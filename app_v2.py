import streamlit as st
import google.generativeai as genai
from google.oauth2 import service_account
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload, MediaIoBaseUpload
from PIL import Image
import io
import json
import tempfile
import time
from google.api_core import exceptions

# --- Î¡Î¥Î˜ÎœÎ™Î£Î•Î™Î£ Î£Î•Î›Î™Î”Î‘Î£ ---
st.set_page_config(page_title="HVAC Smart V-Final (Safe Save)", page_icon="ğŸ¤–", layout="wide")

# --- CSS STYLING ---
st.markdown("""<style>
    #MainMenu {visibility: hidden;} footer {visibility: hidden;} .stDeployButton {display:none;}
    .source-box { background-color: #d1fae5; color: #065f46; padding: 10px; border-radius: 8px; margin-bottom: 10px; border: 1px solid #34d399;}
    .status-box { padding: 10px; border-radius: 8px; margin-bottom: 10px; font-size: 14px; border: 1px solid #ddd; }
</style>""", unsafe_allow_html=True)

# --- GLOBAL CONSTANTS ---
INDEX_FILE_NAME = "hvac_master_index_v10.json"

# --- 1. Î£Î¥ÎÎ”Î•Î£Î— & Î•Î Î™Î›ÎŸÎ“Î— ÎœÎŸÎÎ¤Î•Î›ÎŸÎ¥ (AUTO) ---
auth_status = "â³ ..."
drive_service = None
CURRENT_MODEL_NAME = "gemini-1.5-flash" # Default safe start

try:
    # A. Setup Google AI
    if "GEMINI_KEY" in st.secrets:
        genai.configure(api_key=st.secrets["GEMINI_KEY"])
        
        # --- Î›ÎŸÎ“Î™ÎšÎ— Î‘Î¥Î¤ÎŸÎœÎ‘Î¤Î—Î£ Î•Î Î™Î›ÎŸÎ“Î—Î£ (AUTO-SELECT) ---
        try:
            # 1. Î›Î®ÏˆÎ· ÏŒÎ»Ï‰Î½ Ï„Ï‰Î½ Î¼Î¿Î½Ï„Î­Î»Ï‰Î½
            all_models = [m.name.replace("models/", "") for m in genai.list_models()]
            
            # 2. Î›Î¯ÏƒÏ„Î± Ï€ÏÎ¿Ï„Î¯Î¼Î·ÏƒÎ·Ï‚ (Î‘Ï€ÏŒ Ï„Î¿ ÎºÎ±Î»ÏÏ„ÎµÏÎ¿ ÏƒÏ„Î¿ Ï‡ÎµÎ¹ÏÏŒÏ„ÎµÏÎ¿)
            priority_list = [
                "gemini-2.0-flash-exp", # Experimental New
                "gemini-2.0-flash",     # Stable New (Î±Î½ Î²Î³ÎµÎ¹)
                "gemini-1.5-pro",       # High Intelligence
                "gemini-1.5-flash"      # Fast & Cheap
            ]
            
            # 3. Î•Ï€Î¹Î»Î¿Î³Î®
            detected_model = None
            for wanted in priority_list:
                if wanted in all_models:
                    detected_model = wanted
                    break
            
            if detected_model:
                CURRENT_MODEL_NAME = detected_model
        except Exception as e:
            print(f"Auto-select failed, using default: {e}")

    # B. Setup Google Drive
    if "GCP_SERVICE_ACCOUNT" in st.secrets:
        gcp_raw = st.secrets["GCP_SERVICE_ACCOUNT"].strip()
        if gcp_raw.startswith("'") and gcp_raw.endswith("'"): gcp_raw = gcp_raw[1:-1]
        info = json.loads(gcp_raw)
        if "private_key" in info: info["private_key"] = info["private_key"].replace("\\n", "\n")
        creds = service_account.Credentials.from_service_account_info(
            info, scopes=['https://www.googleapis.com/auth/drive']
        )
        drive_service = build('drive', 'v3', credentials=creds)
        auth_status = "âœ… Online"
except Exception as e:
    auth_status = f"âš ï¸ Error: {str(e)}"

# --- Î’Î‘Î£Î™ÎšÎ•Î£ Î›Î•Î™Î¤ÎŸÎ¥Î¡Î“Î™Î•Î£ DRIVE ---

def load_index():
    """Î¦Î¿ÏÏ„ÏÎ½ÎµÎ¹ Ï„Î¿ JSON Î•Ï…ÏÎµÏ„Î®ÏÎ¹Î¿ Î±Ï€ÏŒ Ï„Î¿ Drive"""
    if not drive_service: return {}
    try:
        results = drive_service.files().list(q=f"name = '{INDEX_FILE_NAME}' and trashed = false", fields="files(id)").execute()
        files = results.get('files', [])
        if files:
            file_id = files[0]['id']
            request = drive_service.files().get_media(fileId=file_id)
            fh = io.BytesIO()
            downloader = MediaIoBaseDownload(fh, request)
            done = False
            while done is False: _, done = downloader.next_chunk()
            return json.loads(fh.getvalue().decode('utf-8'))
    except: pass
    return {} 

def save_index(data):
    """Î‘Ï€Î¿Î¸Î·ÎºÎµÏÎµÎ¹ Ï„Î¿ JSON Î•Ï…ÏÎµÏ„Î®ÏÎ¹Î¿ Ï€Î¯ÏƒÏ‰ ÏƒÏ„Î¿ Drive"""
    if not drive_service: return
    try:
        results = drive_service.files().list(q=f"name = '{INDEX_FILE_NAME}' and trashed = false").execute()
        files = results.get('files', [])
        file_metadata = {'name': INDEX_FILE_NAME, 'mimeType': 'application/json'}
        media = MediaIoBaseUpload(io.BytesIO(json.dumps(data).encode('utf-8')), mimetype='application/json')
        if files:
            drive_service.files().update(fileId=files[0]['id'], media_body=media).execute()
        else:
            drive_service.files().create(body=file_metadata, media_body=media).execute()
    except Exception as e:
        print(f"Save Error: {e}")

def get_all_drive_files_meta():
    """Î¦Î­ÏÎ½ÎµÎ¹ Î»Î¯ÏƒÏ„Î± Î¼Îµ ÎŸÎ›Î‘ Ï„Î± Î±ÏÏ‡ÎµÎ¯Î± Ï„Î¿Ï… Drive (Î³Î¹Î± Ï„Î¿ Sync)"""
    if not drive_service: return []
    all_files = []
    page_token = None
    try:
        while True:
            response = drive_service.files().list(
                q="mimeType != 'application/vnd.google-apps.folder' and trashed = false",
                fields='nextPageToken, files(id, name)',
                pageSize=1000,
                pageToken=page_token
            ).execute()
            all_files.extend(response.get('files', []))
            page_token = response.get('nextPageToken', None)
            if page_token is None: break
        return all_files
    except: return []

def download_temp(file_id, file_name):
    """ÎšÎ±Ï„ÎµÎ²Î¬Î¶ÎµÎ¹ Ï€ÏÎ¿ÏƒÏ‰ÏÎ¹Î½Î¬ Î­Î½Î± Î±ÏÏ‡ÎµÎ¯Î¿ Î³Î¹Î± Î±Î½Î¬Î»Ï…ÏƒÎ·"""
    req = drive_service.files().get_media(fileId=file_id)
    fh = io.BytesIO()
    downloader = MediaIoBaseDownload(fh, req)
    done = False
    while done is False: _, done = downloader.next_chunk()
    suffix = ".pdf" if ".pdf" in file_name.lower() else ".jpg"
    with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
        tmp.write(fh.getvalue())
        return tmp.name

def identify_model_with_ai(file_path):
    """AI Vision: Î’Î»Î­Ï€ÎµÎ¹ Ï„Î·Î½ Ï€ÏÏÏ„Î· ÏƒÎµÎ»Î¯Î´Î± ÎºÎ±Î¹ Î²ÏÎ¯ÏƒÎºÎµÎ¹ Ï„Î¿ ÎœÎ¿Î½Ï„Î­Î»Î¿"""
    try:
        # Î§ÏÎ®ÏƒÎ· Ï„Î¿Ï… Î´Ï…Î½Î±Î¼Î¹ÎºÎ¿Ï Î¼Î¿Î½Ï„Î­Î»Î¿Ï…
        model = genai.GenerativeModel(CURRENT_MODEL_NAME)
        gfile = genai.upload_file(file_path)
        while gfile.state.name == "PROCESSING": 
            time.sleep(0.5)
            gfile = genai.get_file(gfile.name)
        
        prompt = "Î”Î¹Î¬Î²Î±ÏƒÎµ Ï„Î·Î½ Ï€ÏÏÏ„Î· ÏƒÎµÎ»Î¯Î´Î±. Î Î¿Î¹Î± ÎµÎ¯Î½Î±Î¹ Î· ÎœÎ¬ÏÎºÎ± ÎºÎ±Î¹ Ï„Î¿ ÎœÎ¿Î½Ï„Î­Î»Î¿; Î‘Ï€Î¬Î½Ï„Î·ÏƒÎµ ÎœÎŸÎÎŸ Î¼Îµ ÎœÎ¬ÏÎºÎ±/ÎœÎ¿Î½Ï„Î­Î»Î¿. Î‘Î½ Î´ÎµÎ½ Ï†Î±Î¯Î½ÎµÏ„Î±Î¹, Î³ÏÎ¬ÏˆÎµ 'Î†Î³Î½Ï‰ÏƒÏ„Î¿'."
        response = model.generate_content([prompt, gfile])
        return response.text.strip()
    except:
        return "Manual (Auto-detect failed)"

# --- SIDEBAR: SYNC & STATUS ---
with st.sidebar:
    st.header("âš™ï¸ Î”Î¹Î±Ï‡ÎµÎ¯ÏÎ¹ÏƒÎ·")
    st.caption(auth_status)
    
    # Î•ÎÎ”Î•Î™ÎÎ— ÎœÎŸÎÎ¤Î•Î›ÎŸÎ¥ (Î–Ï‰Î½Ï„Î±Î½Î®)
    st.divider()
    st.subheader("ğŸ§  AI Brain Status")
    if "2.0" in CURRENT_MODEL_NAME:
        st.success(f"ğŸš€ Running: **{CURRENT_MODEL_NAME}**")
        st.caption("Next-Gen Speed & Vision")
    elif "pro" in CURRENT_MODEL_NAME:
        st.info(f"ğŸ’ Running: **{CURRENT_MODEL_NAME}**")
        st.caption("High Reasoning")
    else:
        st.warning(f"âš¡ Running: **{CURRENT_MODEL_NAME}**")
        st.caption("Standard Fast Model")
        
    st.divider()

    # Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Index
    if "master_index" not in st.session_state:
        st.session_state.master_index = load_index()
        
    st.subheader("ğŸ”„ Î£Ï…Î³Ï‡ÏÎ¿Î½Î¹ÏƒÎ¼ÏŒÏ‚ (Sync)")
    enable_sync = st.toggle("Î•Î½ÎµÏÎ³Î¿Ï€Î¿Î¯Î·ÏƒÎ· Sync", value=False)
    
    if enable_sync:
        # Î›Î¿Î³Î¹ÎºÎ® Î£Ï…Î³Ï‡ÏÎ¿Î½Î¹ÏƒÎ¼Î¿Ï
        if "drive_snapshot" not in st.session_state:
            with st.spinner("â³ Î›Î®ÏˆÎ· Î»Î¯ÏƒÏ„Î±Ï‚ Î±ÏÏ‡ÎµÎ¯Ï‰Î½ Î±Ï€ÏŒ Drive..."):
                st.session_state.drive_snapshot = get_all_drive_files_meta()
        
        drive_files_map = {f['id']: f['name'] for f in st.session_state.drive_snapshot}
        indexed_ids = set(st.session_state.master_index.keys())
        drive_ids = set(drive_files_map.keys())
        
        new_files_ids = list(drive_ids - indexed_ids)
        deleted_files_ids = list(indexed_ids - drive_ids)
        
        total = len(drive_ids)
        indexed = len(indexed_ids) - len(deleted_files_ids)
        
        st.progress(min(indexed / total if total > 0 else 0, 1.0))
        st.write(f"ğŸ“Š **Index:** {indexed} / {total}")
        
        if new_files_ids:
            st.info(f"ğŸ†• ÎÎ­Î±: {len(new_files_ids)}")
            
            # --- Î£Î—ÎœÎ‘ÎÎ¤Î™ÎšÎ— Î‘Î›Î›Î‘Î“Î— Î•Î”Î©: Processing Batch = 1 ---
            # Î•Ï€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î± 1 Î±ÏÏ‡ÎµÎ¯Î¿Ï… Ï„Î· Ï†Î¿ÏÎ¬ Î³Î¹Î± Î¬Î¼ÎµÏƒÎ· Î±Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ·
            to_process = new_files_ids[:1] 
            
            status_placeholder = st.empty()
            
            for fid in to_process:
                fname = drive_files_map[fid]
                status_placeholder.markdown(f"ğŸ” AI Î‘Î½Î¬Î»Ï…ÏƒÎ·: `{fname}`...")
                try:
                    tmp_path = download_temp(fid, fname)
                    model_info = identify_model_with_ai(tmp_path)
                    st.session_state.master_index[fid] = {"name": fname, "model_info": model_info}
                except Exception as e:
                    print(f"Error {fname}: {e}")
            
            status_placeholder.text("ğŸ’¾ Saving Index...")
            save_index(st.session_state.master_index)
            st.rerun() # Î•Ï€Î±Î½ÎµÎºÎºÎ¯Î½Î·ÏƒÎ· Î³Î¹Î± Ï„Î¿ ÎµÏ€ÏŒÎ¼ÎµÎ½Î¿
            
        elif deleted_files_ids:
            st.warning("ğŸ—‘ï¸ ÎšÎ±Î¸Î±ÏÎ¹ÏƒÎ¼ÏŒÏ‚ Î”Î¹Î±Î³ÏÎ±Î¼Î¼Î­Î½Ï‰Î½...")
            for did in deleted_files_ids:
                del st.session_state.master_index[did]
            save_index(st.session_state.master_index)
            st.rerun()
        else:
            st.success("âœ… System Up to Date")
            if "drive_snapshot" in st.session_state:
                del st.session_state.drive_snapshot

# --- MAIN APP ---
st.title("ğŸ¤– HVAC Smart Expert (Auto-AI)")

# Tabs
tab1, tab2 = st.tabs(["ğŸ’¬ Chat & Î”Î¹Î¬Î³Î½Ï‰ÏƒÎ·", "ğŸ—‚ï¸ Î’Î¬ÏƒÎ· Î”ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½"])

with tab2:
    st.metric("Î£ÏÎ½Î¿Î»Î¿ Manuals", len(st.session_state.master_index))
    st.json(dict(list(st.session_state.master_index.items())[:10]))

with tab1:
    # Î•Ï€Î¹Î»Î¿Î³Î® Î¡ÏŒÎ»Î¿Ï…
    c1, c2, c3 = st.columns(3)
    if "tech_mode" not in st.session_state: st.session_state.tech_mode = "Î¤ÎµÏ‡Î½Î¹ÎºÏŒÏ‚ HVAC"
    if c1.button("â„ï¸ AC"): st.session_state.tech_mode = "Î¤ÎµÏ‡Î½Î¹ÎºÏŒÏ‚ ÎšÎ»Î¹Î¼Î±Ï„Î¹ÏƒÎ¼Î¿Ï"
    if c2.button("ğŸ§Š Î¨ÏÎ¾Î·"): st.session_state.tech_mode = "Î¨Ï…ÎºÏ„Î¹ÎºÏŒÏ‚"
    if c3.button("ğŸ”¥ Î‘Î­ÏÎ¹Î¿"): st.session_state.tech_mode = "Î¤ÎµÏ‡Î½Î¹ÎºÏŒÏ‚ ÎšÎ±Ï…ÏƒÏ„Î®ÏÏ‰Î½"

    # Search Function
    def search_index(query):
        query = query.lower()
        matches = []
        for fid, data in st.session_state.master_index.items():
            full_text = (data['name'] + " " + data['model_info']).lower()
            if query in full_text or any(k in full_text for k in query.split() if len(k)>2):
                matches.append((fid, data))
        return matches[:1] # Î•Ï€Î¹ÏƒÏ„ÏÎ¿Ï†Î® Ï„Î¿Ï… Ï€Î¹Î¿ ÏƒÏ‡ÎµÏ„Î¹ÎºÎ¿Ï

    # Chat History
    if "messages" not in st.session_state: st.session_state.messages = []
    for m in st.session_state.messages:
        with st.chat_message(m["role"]): st.markdown(m["content"])

    # User Input
    user_input = st.chat_input("Î ÎµÏÎ¹Î­Î³ÏÎ±ÏˆÎµ Ï„Î· Î²Î»Î¬Î²Î· Î® Ï„Î¿Î½ ÎºÏ‰Î´Î¹ÎºÏŒ...")

    if user_input:
        st.session_state.messages.append({"role": "user", "content": user_input})
        with st.chat_message("user"): st.markdown(user_input)
        
        with st.chat_message("assistant"):
            found_data = None
            media_items = []
            
            # 1. Î‘Î½Î±Î¶Î®Ï„Î·ÏƒÎ· ÏƒÏ„Î¿ Index
            if st.session_state.master_index:
                hits = search_index(user_input)
                if hits:
                    fid, data = hits[0]
                    found_data = f"{data['model_info']} ({data['name']})"
                    st.markdown(f'<div class="source-box">ğŸ“– Î’ÏÎ­Î¸Î·ÎºÎµ Manual: {found_data}</div>', unsafe_allow_html=True)
                    
                    # ÎšÎ±Ï„Î­Î²Î±ÏƒÎ¼Î± Î³Î¹Î± Ï„Î¿ Chat Context
                    try:
                        with st.spinner("ğŸ“¥ Î¦ÏŒÏÏ„Ï‰ÏƒÎ· manual Î³Î¹Î± Î±Î½Î¬Î»Ï…ÏƒÎ·..."):
                            path = download_temp(fid, data['name'])
                            gf = genai.upload_file(path)
                            while gf.state.name == "PROCESSING": 
                                time.sleep(0.5)
                                gf = genai.get_file(gf.name)
                            media_items.append(gf)
                    except: 
                        st.warning("âš ï¸ Î”ÎµÎ½ Î¼Ï€ÏŒÏÎµÏƒÎ± Î½Î± Î±Î½Î¿Î¯Î¾Ï‰ Ï„Î¿ Î±ÏÏ‡ÎµÎ¯Î¿, ÏƒÏ…Î½ÎµÏ‡Î¯Î¶Ï‰ Î¼Îµ Î³ÎµÎ½Î¹ÎºÎ® Î³Î½ÏÏƒÎ·.")
            
            # 2. Î‘Ï€Î¬Î½Ï„Î·ÏƒÎ· AI (Dynamic Model)
            try:
                model = genai.GenerativeModel(CURRENT_MODEL_NAME)
                
                # Context Prompting
                context_str = f"ÎˆÏ‡ÎµÎ¹Ï‚ Ï„Î¿ manual: {found_data}" if found_data else "Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ ÏƒÏ…Î³ÎºÎµÎºÏÎ¹Î¼Î­Î½Î¿ manual, Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¯Î·ÏƒÎµ Î³ÎµÎ½Î¹ÎºÎ® Î³Î½ÏÏƒÎ·."
                
                full_prompt = f"""
                Î•Î¯ÏƒÎ±Î¹ Î­Î¼Ï€ÎµÎ¹ÏÎ¿Ï‚ {st.session_state.tech_mode}.
                ÎŸÎ”Î—Î“Î™Î•Î£:
                1. {context_str}
                2. Î‘Ï€Î¬Î½Ï„Î·ÏƒÎµ ÏƒÏ„Î± Î•Î»Î»Î·Î½Î¹ÎºÎ¬, ÏƒÏÎ½Ï„Î¿Î¼Î± ÎºÎ±Î¹ Ï„ÎµÏ‡Î½Î¹ÎºÎ¬.
                3. Î‘Î½ ÎµÎ¯Î½Î±Î¹ ÎºÏ‰Î´Î¹ÎºÏŒÏ‚ Î²Î»Î¬Î²Î·Ï‚, Î´ÏÏƒÎµ Ï€Î¹Î¸Î±Î½Î­Ï‚ Î±Î¹Ï„Î¯ÎµÏ‚ ÎºÎ±Î¹ Î»ÏÏƒÎµÎ¹Ï‚.
                
                Î•Î¡Î©Î¤Î—Î£Î— Î¤Î•Î§ÎÎ™ÎšÎŸÎ¥: {user_input}
                """
                
                with st.spinner(f"ğŸ§  Î£ÎºÎ­Ï†Ï„Î¿Î¼Î±Î¹ (Î¼Îµ {CURRENT_MODEL_NAME})..."):
                    resp = model.generate_content([full_prompt, *media_items])
                    st.markdown(resp.text)
                    st.session_state.messages.append({"role": "assistant", "content": resp.text})
                    
            except Exception as e:
                st.error(f"Error: {e}")
